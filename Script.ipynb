{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization of the set into 37, 74, and 148 states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descritezation37(df):\n",
    "    df['Val']=df['Val']*4\n",
    "    df['Down']=df['Down']*4\n",
    "    df['Up']=df['Up']*4\n",
    "    df = df.round({'Val':0, 'Down':0, 'Up':0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descritezation74(df):\n",
    "    df['Val']=df['Val']*8\n",
    "    df['Down']=df['Down']*8\n",
    "    df['Up']=df['Up']*8\n",
    "    df = df.round({'Val':0, 'Down':0, 'Up':0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def descritezation148(df):\n",
    "    df['Val']=df['Val']*16\n",
    "    df['Down']=df['Down']*16\n",
    "    df['Up']=df['Up']*16\n",
    "    df = df.round({'Val':0, 'Down':0, 'Up':0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm(df):\n",
    "    temp = min(min(df['Down']),min(df['Val']))\n",
    "    df['Val']=df['Val']-temp\n",
    "    df['Down']=df['Down']-temp\n",
    "    df['Up']=df['Up']-temp\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation of a chain as a matrix of sequential states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view(df):\n",
    "    temp = df.copy()\n",
    "    temp['step1']=temp['Val'].shift(-1)\n",
    "    temp['step2']=temp['Val'].shift(-2)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the chain into time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def period(df,year1,month1,day1,year2,month2,day2):\n",
    "    temp = df[(df['Date'] >= str(year1) + '-' + str(month1) + '-' + str(day1)) & (df['Date'] <= str(year2) + '-' + str(month2) + '-' + str(day2))]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小alculating chain frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freq(df):\n",
    "    temp = df.copy()\n",
    "    value=temp.sort_values('Val').Val.drop_duplicates().reset_index()\n",
    "    freq_i =temp.groupby('Val').count().reset_index()\n",
    "    freq_i.drop(['Date', 'Up','step1','step2'], axis='columns', inplace=True)\n",
    "    freq_i.rename(columns={'Down': 'Count_freq_i'}, inplace=True)\n",
    "    freq_ij=temp.groupby(['Val','step1']).count().reset_index()\n",
    "    freq_ij.drop(['Date', 'Up','step2'], axis='columns', inplace=True)\n",
    "    freq_ij.rename(columns={'Down': 'Count_freq_ij'}, inplace=True)\n",
    "    freq_ij2=temp.groupby(['Val','step1','step2']).count().reset_index()\n",
    "    freq_ij2.drop(['Date', 'Up',], axis='columns', inplace=True)\n",
    "    freq_ij2.rename(columns={'Down': 'Count_freq_ij2'}, inplace=True)\n",
    "    freq_j=temp.groupby('step1').count().reset_index()\n",
    "    freq_j.drop(['Date', 'Up','Val','step2'], axis='columns', inplace=True)\n",
    "    freq_j.rename(columns={'Down': 'Count_freq_j'}, inplace=True)\n",
    "    freq_0i0=temp.groupby('step1').count().reset_index()\n",
    "    freq_0i0.drop(['Date', 'Up','Val','step2'], axis='columns', inplace=True)\n",
    "    freq_0i0.rename(columns={'Down': 'Count_freq_0i0'}, inplace=True)\n",
    "    freq_0ij=temp.groupby(['step1','step2']).count().reset_index()\n",
    "    freq_0ij.drop(['Date', 'Up','Val'], axis='columns', inplace=True)\n",
    "    freq_0ij.rename(columns={'Down': 'Count_freq_0ij'}, inplace=True)\n",
    "    return freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小alculating statistics to test the independence hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def independent(temp, conf):\n",
    "    N=len(temp.sort_values('Val').Val.drop_duplicates())\n",
    "    n=len(temp)\n",
    "    freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij = freq(temp)\n",
    "    h=temp[['Val','step1']].merge(freq_i,how='left', on='Val',suffixes=['','_freq_i']).merge(freq_j,how='left', on='step1',suffixes=['','_freq_j']).merge(freq_ij,how='left', on=['Val','step1'],suffixes=['','_freq_ij']).drop_duplicates()\n",
    "    h=h.fillna(0) \n",
    "    h['t']=(h.Count_freq_ij-h.Count_freq_i*h.Count_freq_j/n)**2/(h.Count_freq_i*h.Count_freq_j/n)\n",
    "    t=h.t.sum()\n",
    "    t_chi=chi2.ppf(conf, ((N**1-1)*(N-1)) )\n",
    "    return t,t_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小alculating statistics to test chain order hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order1(temp, conf):\n",
    "    freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij = freq(temp)\n",
    "    h=temp[['Val','step1', 'step2']].merge(freq_ij2,how='left',on=['Val', 'step1', 'step2'],suffixes=['','freq_ij2'])\n",
    "    h=h.merge(freq_0i0, how='left', on='step1', suffixes=['','_0i0'])\n",
    "    h=h.merge(freq_0ij,how='left',on=['step1', 'step2'],suffixes=['','freq_0ij'])\n",
    "    h=h.merge(freq_ij ,how='left',on=['Val','step1'],suffixes=['','freq_ij'])\n",
    "    h=h.merge(freq_i, how='left',on=['Val'],suffixes=['','freq_i']).drop_duplicates().rename(columns={'val':'val freq_ij2'})\n",
    "    N = len(temp.sort_values('Val').Val.drop_duplicates())\n",
    "    h['t']=(h.Count_freq_ij-h.Count_freq_i*h.Count_freq_0ij/h.Count_freq_0i0)**2/(h.Count_freq_i*h.Count_freq_0ij/h.Count_freq_0i0)\n",
    "    t = h.t.sum()\n",
    "    t_chi = chi2.ppf(conf, ((N**2-N**1)*(N-1)) )\n",
    "    return t,t_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  小alculating statistics to test the uniformity hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniform(temp, conf):\n",
    "    f1=temp[temp.index<temp.index[0] + len(temp)//2]\n",
    "    f2=temp[temp.index>=temp.index[0] + len(temp)//2]\n",
    "    N = len(temp.Val.drop_duplicates())\n",
    "    d = len(temp[[\"Val\",\"step1\"]].drop_duplicates())\n",
    "    freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij = freq(f1)\n",
    "    h1 = f1[['Val','step1']].merge(freq_i,how='left',on=['Val'],suffixes=['','freq_i']).merge(freq_ij,how='left',on=['Val','step1'],suffixes=['','freq_ij']).drop_duplicates()\n",
    "    freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij = freq(f2)\n",
    "    h2 = f2[['Val','step1']].merge(freq_i,how='left',on=['Val'],suffixes=['','freq_i']).merge(freq_ij,how='left',on=['Val','step1'],suffixes=['','freq_ij']).drop_duplicates()\n",
    "    freq_i,freq_ij,freq_ij2,freq_j,freq_0i0,freq_0ij = freq(temp)\n",
    "    h10 = h1.merge(freq_i,how='left',on=['Val'],suffixes=['','freq_i0']).merge(freq_ij,how='left',on=['Val','step1'],suffixes=['','freq_ij0']).drop_duplicates()\n",
    "    h20 = h2.merge(freq_i,how='left',on=['Val'],suffixes=['','freq_i0']).merge(freq_ij,how='left',on=['Val','step1'],suffixes=['','freq_ij0']).drop_duplicates()\n",
    "    h10['t'] = h10.Count_freq_ij*np.log(h10.Count_freq_ij*h10.Count_freq_ifreq_i0/(h10.Count_freq_i*h10.Count_freq_ijfreq_ij0))\n",
    "    h20['t'] = h20.Count_freq_ij*np.log(h20.Count_freq_ij*h20.Count_freq_ifreq_i0/(h20.Count_freq_i*h20.Count_freq_ijfreq_ij0))\n",
    "    t = h10.t.sum()+h20.t.sum()\n",
    "    t_chi = chi2.ppf(conf, (d-N)*(2-1))\n",
    "    return t,t_chi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing over a 2-year interval with 37 discrete states and a confidence level of 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('Data.xlsx')\n",
    "data['Date'] = pd.to_datetime(data['Date'],format=\"%d.%m.%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = descritezation37(data)\n",
    "temp = norm(temp)\n",
    "temp = period(temp,2012,1,15,2014,1,15)\n",
    "temp = view(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(t, t_chi)\n",
      "(3300.433344864523, 559.3059462065796)\n",
      "(3977.451659336085, 11482.055760113757)\n",
      "(42.36023828774787, 85.95017624510335)\n"
     ]
    }
   ],
   "source": [
    "print(\"(t, t_chi)\")\n",
    "print(independent(temp,0.99))\n",
    "print(order1(temp,0.99))\n",
    "print(uniform(temp,0.99))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
